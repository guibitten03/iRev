SAVE FOLDER： ../dataset/.data/All_Beauty_data_default
Warning: the word embedding file is not provided, will be initialized randomly
2023-12-06 13:21:46: Step1: loading raw review datasets...
===============Start:all  rawData size======================
dataNum: 5264
userNum: 990
itemNum: 85
data densiy: 0.0626
===============End: rawData size========================
------------------------------------------------------------
2023-12-06 13:21:46 Step2: split datsets into train/val/test, save into npy data
===============Start: no-preprocess: trainData size======================
dataNum: 4211
userNum: 989
itemNum: 83
===============End: no-preprocess: trainData size========================
===============Start--process finished: trainData size======================
dataNum: 4219
userNum (config): 990
itemNum (config): 85
===============End-process finished: trainData size========================
2023-12-06 13:21:46
Train data size (config): 4219
Val data size (config): 523
Test data size (config): 522
------------------------------------------------------------
2023-12-06 13:21:46 Step3: Construct the vocab and user/item reviews from training set.
LDA transform matrix: (1075, 32)
The vocab size: 5499
Average user document length: 130.35757575757575
Average item document length: 285.21176470588233
2023-12-06 13:21:47
u_max_r:5
i_max_r:19
r_max_len：63
------------------------------------------------------------
2023-12-06 13:21:47 Step4: padding all the text and id lists and save into npy.
user document length: 500
item document length: 500
------------------------------------------------------------
2023-12-06 13:21:48 start writing npy...
2023-12-06 13:21:48 write finised
------------------------------------------------------------
2023-12-06 13:21:48 Step5: start word embedding mapping...
############################
out of vocab: 5499
w2v size: 5499
############################
Vocab Size and Word Dim: (5499, 300)
2023-12-06 13:21:48 all steps finised, cost time: 1.2391s
