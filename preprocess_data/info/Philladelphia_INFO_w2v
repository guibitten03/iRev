SAVE FOLDER： ../dataset/.data/Philladelphi_data_word2vec
Warning: the word embedding file is not provided, will be initialized randomly
2024-01-12 22:04:03: Step1: loading raw review datasets...
===============Start:all  rawData size======================
dataNum: 967552
userNum: 279857
itemNum: 14569
data densiy: 0.0002
===============End: rawData size========================
------------------------------------------------------------
2024-01-12 22:04:06 Step2: split datsets into train/val/test, save into npy data
===============Start: no-preprocess: trainData size======================
dataNum: 774041
userNum: 244426
itemNum: 14569
===============End: no-preprocess: trainData size========================
===============Start--process finished: trainData size======================
dataNum: 811640
userNum (config): 279857
itemNum (config): 14569
===============End-process finished: trainData size========================
2024-01-12 22:04:20
Train data size (config): 811640
Val data size (config): 77956
Test data size (config): 77956
------------------------------------------------------------
2024-01-12 22:04:20 Step3: Construct the vocab and user/item reviews from training set.
LDA transform matrix: (294426, 32)
The vocab size: 50002
Average user document length: 107.25638093740731
Average item document length: 436.3376347038232
2024-01-12 22:18:29
u_max_r:3
i_max_r:84
r_max_len：86
------------------------------------------------------------
2024-01-12 22:18:31 Step4: padding all the text and id lists and save into npy.
user document length: 500
item document length: 500
------------------------------------------------------------
2024-01-12 22:18:51 start writing npy...
2024-01-12 22:19:01 write finised
------------------------------------------------------------
2024-01-12 22:19:01 Step5: start word embedding mapping...
############################
out of vocab: 10590
w2v size: 50002
############################
Vocab Size and Word Dim: (50002, 300)
2024-01-12 22:19:14 all steps finised, cost time: 911.0659s
